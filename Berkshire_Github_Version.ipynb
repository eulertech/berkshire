{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### This is an web scraping exercise to see what is possible to get from Berkshire Hathaway's shareholder letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T18:31:26.412352Z",
     "start_time": "2019-09-15T18:31:25.903645Z"
    }
   },
   "outputs": [],
   "source": [
    "import certifi # validates trustworthiness of site\n",
    "import pickle # Opening and closing intermediate files.\n",
    "import PyPDF2 # library to get data from PDFs\n",
    "import os # operating system\n",
    "import re # Regular expressions\n",
    "import requests # libary for web scraping\n",
    "import shutil # file operations\n",
    "import urllib3 # Get stuff from the internet\n",
    "import urllib3.contrib.pyopenssl # provides secure connection to site\n",
    "from bs4 import BeautifulSoup # Parsing HTML\n",
    "from tika import parser # parse PDFs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T18:31:26.419613Z",
     "start_time": "2019-09-15T18:31:26.415622Z"
    }
   },
   "outputs": [],
   "source": [
    "# The code below establishes a secure internet connection throughout the scraping process. \n",
    "\n",
    "urllib3.contrib.pyopenssl.inject_into_urllib3()\n",
    "https = urllib3.PoolManager(cert_reqs='CERT_REQUIRED', ca_certs=certifi.where(), timeout=15.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T18:31:26.435567Z",
     "start_time": "2019-09-15T18:31:26.422615Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gets url of years from 1977 to 1997 (first column of letters on website)\n",
    "\n",
    "url_years = [f'https://www.berkshirehathaway.com/letters/{year}.html' for year in range(1977, 1998)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T18:31:27.424289Z",
     "start_time": "2019-09-15T18:31:26.438559Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saves letters into a list.\n",
    "\n",
    "annual_letters = []\n",
    "for i in range(len(url_years)):\n",
    "    annual_letters.append(https.request('GET', url_years[i]).data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T18:31:27.701309Z",
     "start_time": "2019-09-15T18:31:27.424289Z"
    }
   },
   "outputs": [],
   "source": [
    "# Writes letters to file.\n",
    "\n",
    "base_dir = # put your directory here.\n",
    "count = 0\n",
    "for year in range(1977, 1998):\n",
    "    with open(base_dir + '/' + f'{year}_letter.txt',\"wb\") as f:\n",
    "        f.write(annual_letters[count])\n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The letters from 1998 to 2018 are in PDF format. The following code attempts to scrape text from PDF files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T18:31:27.712284Z",
     "start_time": "2019-09-15T18:31:27.703306Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1.) Create directory for each year of files. Save split PDF files in each directory.\n",
    "\n",
    "# Make a list of years where each year is a year with an annual report:\n",
    "years = [str(year) for year in range(1998, 2019)]\n",
    "import os\n",
    "\n",
    "# Note: Change output directory to something else on laptop!\n",
    "output_dir = # put directory here to put files for PDF pages\n",
    "for year in years:\n",
    "    year_directory = os.path.join(output_dir, year)\n",
    "    os.makedirs(year_directory, mode=0o777, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T18:31:27.722581Z",
     "start_time": "2019-09-15T18:31:27.715159Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2.) Get PDF file from internet.\n",
    "\n",
    "def download_pdf(url, filename):\n",
    "    import certifi\n",
    "    import shutil\n",
    "    import urllib3\n",
    "    import urllib3.contrib.pyopenssl\n",
    "    \n",
    "    urllib3.contrib.pyopenssl.inject_into_urllib3()\n",
    "    c = urllib3.PoolManager(cert_reqs='CERT_REQUIRED', ca_certs=certifi.where())\n",
    "    \n",
    "    with c.request('GET', url, preload_content=False) as resp, open(filename, 'wb') as out_file:\n",
    "        shutil.copyfileobj(resp, out_file)\n",
    "\n",
    "    resp.release_conn()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T18:31:27.747776Z",
     "start_time": "2019-09-15T18:31:27.724579Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3.) Split PDF file. Save file in year directory.\n",
    "\n",
    "def pdf_splitter(pdfFile):\n",
    "    # creating a pdf file object \n",
    "    pdfFileObj = open(pdfFile, 'rb') \n",
    "\n",
    "    # creating a pdf reader object \n",
    "    pdfReader = PyPDF2.PdfFileReader(pdfFileObj) \n",
    "    for pg in range(pdfReader.numPages):\n",
    "        if len(str(pg)) == 1:\n",
    "            filename = pdfFile.split('.')[0] + '_0' + str(pg) + '.pdf'\n",
    "        else:\n",
    "            filename = pdfFile.split('.')[0] + '_' + str(pg) + '.pdf'\n",
    "        pageObj = pdfReader.getPage(pg)\n",
    "        pdfWriter = PyPDF2.PdfFileWriter()\n",
    "        pdfWriter.addPage(pageObj)\n",
    "        # new pdf file object \n",
    "        newFile = open(filename, 'wb') \n",
    "        pdfWriter.write(newFile)\n",
    "\n",
    "    pdfFileObj.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T18:31:27.762741Z",
     "start_time": "2019-09-15T18:31:27.749791Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4.) Loop through every page to get text. Write text file to new file.\n",
    "\n",
    "def pdf_reader(pdffile):\n",
    "    raw = parser.from_file(pdffile, xmlContent=True)['content']\n",
    "    data = BeautifulSoup(raw, features='html')\n",
    "    message = data.find(class_='page').encode('utf-8') # for first page\n",
    "    return(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T18:31:38.751954Z",
     "start_time": "2019-09-15T18:31:27.765728Z"
    }
   },
   "outputs": [],
   "source": [
    "# Put it all together.\n",
    "\n",
    "def main():\n",
    "    \n",
    "    base_dir = # the root folder where PDF files saved \n",
    "    all_pdf_dict = dict()\n",
    "    urls = ['https://www.berkshirehathaway.com/letters/1998pdf.pdf',\n",
    "                'https://www.berkshirehathaway.com/letters/final1999pdf.pdf',\n",
    "                'https://www.berkshirehathaway.com/letters/2000pdf.pdf',\n",
    "                'https://www.berkshirehathaway.com/letters/2001pdf.pdf',\n",
    "                'https://www.berkshirehathaway.com/letters/2002pdf.pdf']\n",
    "    urls2 = [f'https://www.berkshirehathaway.com/letters/{year}ltr.pdf' for year in range(2003, 2019)]\n",
    "    urls_combined = urls + urls2\n",
    "    \n",
    "    for year in range(1998, 2019):\n",
    "        count = 0\n",
    "        one_pdf_dict = dict()\n",
    "        year_pdf = f'{year}.pdf'\n",
    "        year_dir = base_dir + '/' + str(year)\n",
    "        complete_letter = base_dir + '/' + str(year) + '/' + year_pdf\n",
    "        \n",
    "        for root, dirs, files in os.walk(year_dir):\n",
    "            for name in files:\n",
    "                filename = root + '/' + name\n",
    "                if filename != complete_letter:\n",
    "                    count = count + 1\n",
    "                    one_pdf_dict[str(count)] = pdf_reader(filename)\n",
    "        \n",
    "        all_pdf_dict[str(year)] = one_pdf_dict\n",
    "        \n",
    "        with open(base_dir + '/' + f'{year}_letter.txt',\"w\") as f:\n",
    "            f.write(str(one_pdf_dict))\n",
    "    \n",
    "    return(all_pdf_dict)\n",
    "\n",
    "main()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
