{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tom Halloin <br> Springboard Data Science Career Track <br>\n",
    "\n",
    "<h1 align=\"center\">Capstone Project 2: Analysis of Berkshire Hathaway Shareholder Letters Using Natural Language Processing (NLP) Techniques</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T02:05:15.807118Z",
     "start_time": "2019-12-12T02:05:15.791255Z"
    }
   },
   "source": [
    "<h3 align=\"center\">Part 1: Scraping Data</h3> <br>\n",
    "\n",
    "The data comes from Berkshire Hathawayâ€™s shareholder letters available at the link https://www.berkshirehathaway.com/letters/letters.html. The letters come in both HTML and PDF format, so part of the challenge will be scraping data from both formats into a feasible data set.\n",
    "\n",
    "<b>NOTE: Berkshire eventually caught on that I was a bot and has denied me scraping access. I do have the raw letters in a file on my laptop from before the denial that I will upload with the milestone report.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:00:46.555145Z",
     "start_time": "2019-10-17T18:00:46.064805Z"
    }
   },
   "outputs": [],
   "source": [
    "import certifi  # validates trustworthiness of site\n",
    "import pickle  # Opening and closing intermediate files\n",
    "import PyPDF2  # library to get data from PDFs\n",
    "import os  # operating system\n",
    "import re  # Regular expressions\n",
    "import requests  # libary for web scraping\n",
    "import shutil  # file operations\n",
    "import urllib3  # Get stuff from the internet\n",
    "import urllib3.contrib.pyopenssl  # provides secure connection to site\n",
    "from bs4 import BeautifulSoup  # Parsing HTML\n",
    "from tika import parser  # parse PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:00:46.562152Z",
     "start_time": "2019-10-17T18:00:46.557127Z"
    }
   },
   "outputs": [],
   "source": [
    "# The code below establishes a secure internet connection throughout the scraping process.\n",
    "\n",
    "urllib3.contrib.pyopenssl.inject_into_urllib3()\n",
    "https = urllib3.PoolManager(\n",
    "    cert_reqs='CERT_REQUIRED', ca_certs=certifi.where(), timeout=15.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:00:46.569094Z",
     "start_time": "2019-10-17T18:00:46.564107Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gets url of years from 1977 to 1997 (first column of letters on website)\n",
    "\n",
    "url_years = [\n",
    "    f'https://www.berkshirehathaway.com/letters/{year}.html' for year in range(1977, 1998)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:00:47.493629Z",
     "start_time": "2019-10-17T18:00:46.571089Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saves letters into a list.\n",
    "\n",
    "annual_letters = []\n",
    "for i in range(len(url_years)):\n",
    "    annual_letters.append(https.request('GET', url_years[i]).data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:00:47.557137Z",
     "start_time": "2019-10-17T18:00:47.497352Z"
    }
   },
   "outputs": [],
   "source": [
    "# Writes letters to file.\n",
    "\n",
    "base_dir = 'C:/Users/Tom/Documents/Berkshire/pdf_files/raw_letters'\n",
    "count = 0\n",
    "for year in range(1977, 1998):\n",
    "    with open(base_dir + '/' + f'{year}_letter.txt', \"wb\") as f:\n",
    "        f.write(annual_letters[count])\n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The letters from 1998 to 2018 are in PDF format. The following code attempts to scrape text from PDF files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:00:47.599022Z",
     "start_time": "2019-10-17T18:00:47.566112Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1.) Create directory for each year of files. Save split PDF files in each directory.\n",
    "\n",
    "# Make a list of years where each year is a year with an annual report:\n",
    "import os\n",
    "years = [str(year) for year in range(1998, 2019)]\n",
    "\n",
    "# Note: Change output directory to something else on laptop!\n",
    "output_dir = 'C:/Users/Tom/Documents/Berkshire/pdf_files'\n",
    "for year in years:\n",
    "    year_directory = os.path.join(output_dir, year)\n",
    "    os.makedirs(year_directory, mode=0o777, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:00:47.634928Z",
     "start_time": "2019-10-17T18:00:47.613985Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2.) Get PDF file from internet.\n",
    "\n",
    "def download_pdf(url, filename):\n",
    "    import certifi\n",
    "    import shutil\n",
    "    import urllib3\n",
    "    import urllib3.contrib.pyopenssl\n",
    "    \n",
    "    urllib3.contrib.pyopenssl.inject_into_urllib3()\n",
    "    c = urllib3.PoolManager(cert_reqs='CERT_REQUIRED', ca_certs=certifi.where())\n",
    "    \n",
    "    with c.request('GET', url, preload_content=False) as resp, open(filename, 'wb') as out_file:\n",
    "        shutil.copyfileobj(resp, out_file)\n",
    "\n",
    "    resp.release_conn()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:00:47.660855Z",
     "start_time": "2019-10-17T18:00:47.644900Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3.) Split PDF file. Save file in year directory.\n",
    "\n",
    "\n",
    "def pdf_splitter(pdfFile):\n",
    "    # creating a pdf file object\n",
    "    pdfFileObj = open(pdfFile, 'rb')\n",
    "\n",
    "    # creating a pdf reader object\n",
    "    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "    for pg in range(pdfReader.numPages):\n",
    "        if len(str(pg)) == 1:\n",
    "            filename = pdfFile.split('.')[0] + '_0' + str(pg) + '.pdf'\n",
    "        else:\n",
    "            filename = pdfFile.split('.')[0] + '_' + str(pg) + '.pdf'\n",
    "        pageObj = pdfReader.getPage(pg)\n",
    "        pdfWriter = PyPDF2.PdfFileWriter()\n",
    "        pdfWriter.addPage(pageObj)\n",
    "        # new pdf file object\n",
    "        newFile = open(filename, 'wb')\n",
    "        pdfWriter.write(newFile)\n",
    "\n",
    "    pdfFileObj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:00:47.672824Z",
     "start_time": "2019-10-17T18:00:47.664850Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4.) Loop through every page to get text. Write text file to new file.\n",
    "\n",
    "\n",
    "def pdf_reader(pdffile):\n",
    "    raw = parser.from_file(pdffile, xmlContent=True)['content']\n",
    "    data = BeautifulSoup(raw, features='html')\n",
    "    message = data.find(class_='page').encode('utf-8')  # for first page\n",
    "    return(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:00:54.521231Z",
     "start_time": "2019-10-17T18:00:47.676813Z"
    }
   },
   "outputs": [],
   "source": [
    "# Put it all together.\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    base_dir = 'C:/Users/Tom/Documents/Berkshire/pdf_files'\n",
    "    all_pdf_dict = dict()\n",
    "    urls = ['https://www.berkshirehathaway.com/letters/1998pdf.pdf',\n",
    "            'https://www.berkshirehathaway.com/letters/final1999pdf.pdf',\n",
    "            'https://www.berkshirehathaway.com/letters/2000pdf.pdf',\n",
    "            'https://www.berkshirehathaway.com/letters/2001pdf.pdf',\n",
    "            'https://www.berkshirehathaway.com/letters/2002pdf.pdf']\n",
    "    urls2 = [\n",
    "        f'https://www.berkshirehathaway.com/letters/{year}ltr.pdf' for year in range(2003, 2019)]\n",
    "    urls_combined = urls + urls2\n",
    "\n",
    "    for year in range(1998, 2019):\n",
    "        count = 0\n",
    "        one_pdf_dict = dict()\n",
    "        year_pdf = f'{year}.pdf'\n",
    "        year_dir = base_dir + '/' + str(year)\n",
    "        complete_letter = base_dir + '/' + str(year) + '/' + year_pdf\n",
    "\n",
    "        for root, dirs, files in os.walk(year_dir):\n",
    "            for name in files:\n",
    "                filename = root + '/' + name\n",
    "                if filename != complete_letter:\n",
    "                    count = count + 1\n",
    "                    one_pdf_dict[str(count)] = pdf_reader(filename)\n",
    "\n",
    "        all_pdf_dict[str(year)] = one_pdf_dict\n",
    "\n",
    "        with open(base_dir + '/raw_letters/' + f'{year}_letter.txt', \"w\") as f:\n",
    "            f.write(str(one_pdf_dict))\n",
    "\n",
    "    return(all_pdf_dict)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
